---
{"aliases":["Random Forest"],"dg-publish":true,"dg-path":"人工智能/机器学习/随机森林.md","tags":["ML"],"permalink":"/人工智能/机器学习/随机森林/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-08-28T21:53:13.860+08:00","updated":"2025-11-15T09:29:57.573+08:00"}
---


(terminology::**Random Forest**)
> 随机森林是一种强大的[[集成学习\|集成学习]]算法，属于**Bagging (套袋法)** 的范畴。它通过构建多棵**[[决策树\|决策树]]**，并结合它们的预测结果来提高模型的准确性、稳定性和鲁棒性。随机森林在分类和回归任务中都表现出色，是[[机器学习\|机器学习]]领域最受欢迎的算法之一。


![Pasted image 20241224005211.png](../img/user/Functional%20files/Photo%20Resources/Pasted%20image%2020241224005211.png)

### 核心思想：多棵树的集体智慧

随机森林的核心思想是“三个臭皮匠，顶个诸葛亮”。它不是训练一棵强大的决策树，而是训练多棵相对较弱但具有**多样性**的决策树，然后通过**投票（分类）**或**平均（回归）**的方式来整合它们的预测。这种集成方法能够显著降低模型的**方差**，从而有效防止[[过拟合\|过拟合]]。

### 两大随机性来源

随机森林之所以“随机”，主要体现在以下两个方面：

1.  **数据随机化 (Bagging)**:
    -   **原理**: 通过**自助采样 (Bootstrap Aggregating)**，从原始训练集中有放回地随机抽取 $N$ 个样本，生成 $K$ 个不同的训练子集。每棵决策树都在一个独立的训练子集上进行训练。
    - **作用**: 确保每棵树看到的数据略有不同，从而增加了树之间的多样性。

2.  **特征随机化 (Feature Randomness)**:
    -   **原理**: 在每棵决策树的每个节点进行分裂时，不是从所有可用特征中选择最优特征，而是从一个**随机选择的特征子集**中选择最优特征进行分裂。
    - **作用**: 进一步增加了树之间的多样性，减少了树之间的相关性。这尤其重要，因为如果有一个非常强的特征，所有树都可能倾向于使用它进行分裂，导致树之间变得相似，从而降低了集成的效果。

### 算法流程

#### 1. 随机森林的构建

1.  **自助采样**: 从包含 $N$ 个样本的原始训练集中，有放回地随机抽取 $N$ 个样本，得到一个训练子集。重复此步骤 $K$ 次，得到 $K$ 个不同的训练子集。
2.  **独立建树**: 对于每个训练子集，独立地训练一棵[[决策树\|决策树]]。在每棵树的每个节点分裂时，从所有特征中随机选择 $m$ 个特征（通常 $m \ll$ 总特征数），然后从这 $m$ 个特征中选择最优特征进行分裂。
3.  **不剪枝**: 每棵决策树都尽可能完全生长，不进行剪枝（因为集成可以弥补单棵树的过拟合）。

#### 2. 随机森林的预测

-   **分类任务**: 对于一个新的样本，将其输入到所有 $K$ 棵决策树中，每棵树都会给出一个分类结果。最终的预测结果是所有树的**多数投票**。
-   **回归任务**: 对于一个新的样本，将其输入到所有 $K$ 棵决策树中，每棵树都会给出一个预测值。最终的预测结果是所有树的预测值的**平均值**。

### 优缺点分析

| 优点 (Pros)                                  | 缺点 (Cons)                                                                  |
|----------------------------------------------|------------------------------------------------------------------------------|
| **高精度**：通常比单棵决策树具有更高的预测精度。 | **可解释性差**：由于是多棵树的集成，模型内部的决策过程不如单棵决策树直观。 |
| **鲁棒性强**：对噪声和异常值不敏感。         | **计算量大**：需要训练多棵树，计算成本和存储成本相对较高。                 |
| **能处理高维数据**：通过特征随机化，能有效处理特征数量远大于样本数量的情况。 | **训练时间长**：虽然并行化，但总体训练时间仍可能较长。                     |
| **不易过拟合**：通过集成和随机化，有效降低了方差。 |                                                                              |
| **能评估特征重要性**：可以根据特征在每棵树中的贡献来评估其重要性。 |                                                                              |

### OOB (Out-of-Bag) 误差

在自助采样过程中，大约有36.8%的样本不会被任何一棵树抽到。这些未被抽到的样本被称为**袋外样本 (Out-of-Bag, OOB) 样本**。随机森林可以使用这些OOB样本来对模型进行**无偏估计**，而无需额外的交叉验证或独立的测试集。这使得随机森林的评估非常高效。

### 特征重要性 (Feature Importance)

随机森林可以自然地评估特征的重要性。通常有两种方法：

1.  **基于基尼不纯度**: 统计每个特征在所有决策树中作为分裂特征时，所带来的基尼不纯度（或信息增益）的平均减少量。减少量越大，特征越重要。
2.  **基于置换**: 随机打乱某个特征的值，然后观察模型性能（如准确率）的下降程度。性能下降越大，特征越重要。
