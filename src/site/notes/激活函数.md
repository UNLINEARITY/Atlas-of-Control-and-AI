---
{"tags":["Nonlinear","Function"],"dg-publish":true,"dg-path":"人工智能/机器学习/激活函数.md","permalink":"/人工智能/机器学习/激活函数/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-05-21T15:20:28.460+08:00","updated":"2025-08-30T17:25:01.078+08:00"}
---


(terminology::**Activation Function**)
激活函数是用于人工[[神经网络\|神经网络]]中的一种函数，它决定了神经元的输出是否应该被激活，从而引入非线性因素，使神经网络能够学习和表示复杂的数据模式。

决定了**神经元输出**，给网络引入**非线性能力**。  
如果没有激活函数，深度网络只相当于线性变换堆叠，无法拟合复杂模式。

激活函数的主要作用是引入非线性，使得神经网络能够处理非线性问题。没有激活函数，神经网络只是线性模型的叠加，无法解决复杂的任务。激活函数使得神经网络能够逼近任何复杂的函数，从而在图像识别、自然语言处理等任务中取得优异的表现。


### 为什么需要激活函数？

如果没有激活函数（或者只使用线性激活函数），无论[[神经网络\|神经网络]]有多少层，整个网络都将退化为一个简单的线性模型。这意味着网络只能学习线性关系，无法处理非线性可分的数据。激活函数通过引入非线性，赋予了神经网络学习复杂模式的能力，使其能够逼近任何连续函数（根据[[通用近似定理\|通用近似定理]]）。


### 常见激活函数
#### Sigmoid (Logistic Sigmoid)
数学表达式：
$$\begin{align}
\sigma(x)=\frac1{1+e^{-x}}\in(0,1)\quad \sigma^{\prime}(x)=\sigma(x)(1-\sigma(x))
\end{align}$$

早期神经网络（比如最早的多层感知机 MLP）
**输出概率**（如二分类模型最后一层）

**优点**: 将输出压缩到 $(0, 1)$，可解释为概率；平滑可导。


**缺点**：
- 梯度消失问题：当 $x$ 非常大或小，梯度趋近于0。
- 非零均值，导致更新不稳定。
- **计算成本**: 涉及指数运算，相对耗时。

#### Tanh (Hyperbolic Tangent)
$$\begin{align}
\tanh(x)= \dfrac{e^{ x }-e^{ x }}{e^{ x }+e^{ x }}\in (-1,1)\quad \tanh'(x)=1-\tanh ^{2}(x)
\end{align}$$

将输入值压缩到-1到1之间
Sigmoid和Tanh函数用于控制记忆单元的输入、输出和遗忘门，使模型能够捕捉长序列依赖关系。
- RNN（循环神经网络）中很常见
- 相比Sigmoid，Tanh输出均值为0，收敛更快

同样存在梯度消失问题。




#### ReLU（Rectified Linear Unit ）

$$\begin{align}
ReLU(x)=max(0,x) \quad  ReLU'(x)=\begin{cases}
1 ,\;x>0\\ \\

0,\;x\leq0
\end{cases}
\end{align}$$


对于输入值为负时输出为0，正值时输出为该值本身。

- CNN（卷积神经网络）普遍使用
- 各类Transformer（Feed Forward层里）
- 提高训练速度，不容易梯度消失

**缺点**：死神经（Dead Neuron）问题：输入小于0时梯度为0，永远不会更新。


- **优点**: 
    - **计算简单**: 只需要一个阈值判断，计算效率高。
    - **缓解梯度消失**: 当 $x > 0$ 时，导数恒为 1，有效缓解了深层网络的梯度消失问题，加速收敛。
    - **稀疏激活**: 负值输入直接输出 0，导致部分神经元“死亡”，产生稀疏激活，有助于减少[[过拟合\|过拟合]]。
- **缺点**: 
    - **死亡 ReLU (Dying ReLU)**: 当输入为负时，梯度为 0，导致神经元不再更新权重，永远“死亡”。
    - **输出非零均值**。

#### Leaky ReLU
$$\begin{cases}
x & \text{if } x \geq 0 \\ \alpha x & \text{if } x < 0 
\end{cases}$$

- 解决 ReLU 的 Dead Neuron 问题。
- GANs中比较常用。

为解决“死亡 ReLU”问题，提出了多种 ReLU 的变体：

- **Leaky ReLU**: $f(x) = \max(\alpha x, x)$，其中 $\alpha$ 是一个很小的正数（如 0.01）。当 $x < 0$ 时，输出一个小的非零斜率，避免神经元完全死亡。
- **PReLU (Parametric ReLU)**: $f(x) = \max(\alpha x, x)$，其中 $\alpha$ 是一个可学习的参数，而不是固定的常数。
- **ELU (Exponential Linear Unit)**: $f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \le 0 \end{cases}$。在负值区域有平滑的曲线，且输出均值接近 0，有助于加速学习。


#### Softmax 函数

- **公式**: 对于一个包含 $K$ 个类别的输出向量 $z = [z_1, z_2, \dots, z_K]^T$，Softmax 函数将其转换为一个概率分布 $P = [p_1, p_2, \dots, p_K]^T$，其中 $p_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}$。
- **图像**: 将多个输入值转换为和为 1 的概率分布。
- **优点**: 适用于**多分类任务的输出层**，确保所有输出概率之和为 1。
- **缺点**: 仅用于输出层，不适合作为隐藏层的激活函数。
### 激活函数的选择原则
- **隐藏层**: 
    - **ReLU 及其变体**（Leaky ReLU, PReLU, ELU）是目前最常用的选择，因为它们能有效缓解梯度消失问题，计算效率高。
    - 对于需要更平滑输出或对负值有特定处理的场景，ELU 可能是一个好选择。
- **输出层**: 
    - **二分类**: [[Sigmoid函数\|Sigmoid函数]]。
    - **多分类**: Softmax 函数。
    - **回归**: 通常不使用激活函数（或使用线性激活函数），因为输出需要是任意实数。