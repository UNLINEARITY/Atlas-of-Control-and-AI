---
{"tags":["Nonlinear","Function"],"dg-publish":true,"dg-path":"人工智能/机器学习/激活函数.md","permalink":"/人工智能/机器学习/激活函数/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-05-21T15:20:28.460+08:00","updated":"2025-04-29T11:31:00.585+08:00"}
---


(terminology::**Activation Function**)
激活函数是用于人工[[神经网络\|神经网络]]中的一种函数，它决定了神经元的输出是否应该被激活，从而引入非线性因素，使神经网络能够学习和表示复杂的数据模式。

决定了**神经元输出**，给网络引入**非线性能力**。  
如果没有激活函数，深度网络只相当于线性变换堆叠，无法拟合复杂模式。

激活函数的主要作用是引入非线性，使得神经网络能够处理非线性问题。没有激活函数，神经网络只是线性模型的叠加，无法解决复杂的任务。激活函数使得神经网络能够逼近任何复杂的函数，从而在图像识别、自然语言处理等任务中取得优异的表现。
### Sigmoid
数学表达式：
$$\begin{align}
\sigma(x)=\frac1{1+e^{-x}}\in(0,1)\quad \sigma^{\prime}(x)=\sigma(x)(1-\sigma(x))
\end{align}$$

早期神经网络（比如最早的多层感知机 MLP）
**输出概率**（如二分类模型最后一层）

**缺点**：
- 梯度消失问题：当 $x$ 非常大或小，梯度趋近于0。
- 非零均值，导致更新不稳定。
### Tanh 双曲正切
$$\begin{align}
\tanh(x)= \dfrac{e^{ x }-e^{ x }}{e^{ x }+e^{ x }}\in (-1,1)\quad \tanh'(x)=1-\tanh ^{2}(x)
\end{align}$$

将输入值压缩到-1到1之间
Sigmoid和Tanh函数用于控制记忆单元的输入、输出和遗忘门，使模型能够捕捉长序列依赖关系。
- RNN（循环神经网络）中很常见
- 相比Sigmoid，Tanh输出均值为0，收敛更快

同样存在梯度消失问题。

### ReLU
Rectified Linear Unit 
$$\begin{align}
ReLU(x)=max(0,x) \quad  ReLU'(x)=\begin{cases}
1 ,\;x>0\\ \\

0,\;x\leq0
\end{cases}
\end{align}$$


对于输入值为负时输出为0，正值时输出为该值本身。

- CNN（卷积神经网络）普遍使用
- 各类Transformer（Feed Forward层里）
- 提高训练速度，不容易梯度消失

**缺点**：死神经（Dead Neuron）问题：输入小于0时梯度为0，永远不会更新。
### Leaky ReLU
$$\begin{cases}
x & \text{if } x \geq 0 \\ \alpha x & \text{if } x < 0 
\end{cases}$$

- 解决 ReLU 的 Dead Neuron 问题。
- GANs中比较常用。


