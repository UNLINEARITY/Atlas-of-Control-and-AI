---
{"dg-publish":true,"dg-path":"人工智能/机器学习/损失函数.md","permalink":"/人工智能/机器学习/损失函数/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-04-29T00:13:49.617+08:00","updated":"2025-04-29T11:32:45.564+08:00"}
---




| 类型       | 常见损失函数                                   | 适用场景                    |
| -------- | ---------------------------------------- | ----------------------- |
| 回归（数值预测） | MSE、MAE、Huber Loss、Smooth L1 Loss        | 连续数值输出                  |
| 分类（离散预测） | Cross-Entropy Loss、Focal Loss、Hinge Loss | 分类任务（Softmax / Sigmoid） |
| 排序 / 生成  | Triplet Loss、Contrastive Loss、CTC Loss   | 检索、匹配、序列预测              |

### 回归问题
#### 均方误差 MSE 
Mean Squared Error 
基础思想:预测值和真实值的差平方后求平均，惩罚大误差。
$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

- $y_i$: 真实值
- $\hat{y}_i$: 预测值
- $n$: 样本数量
特点:
- 对大误差特别敏感（平方放大）。
- 常用于一般的回归任务。

应用场景：预测房价、温度、时间序列数值等。

#### 平均绝对误差 MAE
Mean Absolute Error
基础思想：预测值和真实值的差的绝对值后求平均，惩罚整体偏差。
$$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

特点:
- 对异常值鲁棒（比 MSE 好）。
- 但不可导于 0 处（影响优化）。

应用场景:对异常值不敏感时，做稳健回归。

####  Huber Loss（平滑版 MSE+MAE）
基础思想:
小误差用 MSE, 大误差用 MAE, 兼顾两者优点。
数学公式:
$$L_{\delta}(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2, & \text{if } |y - \hat{y}| \leq \delta \\
\delta(|y - \hat{y}| - \frac{1}{2}\delta), & \text{otherwise}
\end{cases}$$

$\delta$ 超参数, 控制切换点。
特点:
- 小误差: 像 MSE
- 大误差: 像 MAE
应用场景:
- 自然界数据回归 (如金融数据), 既有少量大误差也有大量小误差。
### 分类问题
#### 交叉熵损失（Cross Entropy Loss）
- 用于**多分类**，配合Softmax。
- 也有**二分类版**（Binary Cross Entropy，配合Sigmoid）。
#### Focal Loss（焦点损失）
基础思想：为了解决类别极度不平衡问题，让模型关注难分类样本。
$$\text{FocalLoss}(p_t) = -\alpha(1 - p_t)^{\gamma} \log(p_t)$$
- $p_t$: 预测正确类别的概率。
- $\alpha$、$\gamma$: 调节参数。

特点：
- $\gamma > 0$ 抑制容易分类的样本。
- 强化难样本的学习。

应用场景：检测极小目标（如目标检测中的 RetinaNet）

### 排序 / 生成


