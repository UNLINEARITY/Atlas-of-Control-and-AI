---
{"dg-publish":true,"dg-path":"人工智能/机器学习/损失函数.md","permalink":"/人工智能/机器学习/损失函数/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-04-29T00:13:49.617+08:00","updated":"2025-08-30T17:31:18.639+08:00"}
---


(terminology::**Loss Function**)
也称为成本函数（Cost Function）或目标函数（Objective Function）

用于**量化模型预测值与真实值之间差异**的函数。在模型训练过程中，我们通过最小化损失函数来优化模型参数，从而使模型的预测尽可能地接近真实值。

| 类型       | 常见损失函数                                   | 适用场景                    |
| -------- | ---------------------------------------- | ----------------------- |
| 回归（数值预测） | MSE、MAE、Huber Loss、Smooth L1 Loss        | 连续数值输出                  |
| 分类（离散预测） | Cross-Entropy Loss、Focal Loss、Hinge Loss | 分类任务（Softmax / Sigmoid） |
| 排序 / 生成  | Triplet Loss、Contrastive Loss、CTC Loss   | 检索、匹配、序列预测              |

### 回归问题 Regression
#### 均方误差 MSE 
**Mean Squared Error** 
基础思想:预测值和真实值的差平方后求平均，惩罚大误差。计算预测值与真实值之差的平方的平均值。对较大的误差惩罚更重，因此对异常值（outliers）比较敏感。损失函数是凸的，易于优化。
$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

- $y_i$: 真实值
- $\hat{y}_i$: 预测值
- $n$: 样本数量
特点:
- 对大误差特别敏感（平方放大）。
- 常用于一般的回归任务。

**适用场景**: 大多数回归问题，当异常值不被认为是噪声时。预测房价、温度、时间序列数值等。


#### 平均绝对误差 MAE
**Mean Absolute Error**
基础思想：预测值和真实值的差的绝对值后求平均，惩罚整体偏差。
$$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

计算预测值与真实值之差的绝对值的平均值。对异常值不那么敏感，因为它对误差的惩罚是线性的。但其在 0 点不可导，优化相对困难。


特点:
- 对异常值鲁棒（比 MSE 好）。
- 但不可导于 0 处（影响优化）。

应用场景:对异常值不敏感时，做稳健回归。
**适用场景**: 当数据中存在较多异常值，且不希望模型过度关注这些异常值时。
####  Huber Loss（平滑版 MSE+MAE）
基础思想:
小误差用 MSE, 大误差用 MAE, 兼顾两者优点。结合了 MSE 和 MAE 的优点。当误差较小时使用 MSE（平方误差），当误差较大时使用 MAE（绝对误差）。


数学公式:
$$L_{\delta}(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2, & \text{if } |y - \hat{y}| \leq \delta \\
\delta(|y - \hat{y}| - \frac{1}{2}\delta), & \text{otherwise}
\end{cases}$$

$\delta$ 超参数, 控制切换点。
特点:
- 小误差: 像 MSE
- 大误差: 像 MAE
 对小误差是二次的，对大误差是线性的，因此对异常值具有鲁棒性，同时在 0 点可导。


应用场景:
- 当数据中可能存在异常值，但又希望在误差较小时保持 MSE 的平滑性时。，自然界数据回归 (如金融数据), 既有少量大误差也有大量小误差。
### 分类问题 Classification
#### 交叉熵损失（Cross Entropy Loss）
- 用于**多分类**，配合Softmax。
- 也有**二分类版**（Binary Cross Entropy，配合Sigmoid）。

**二元交叉熵 (Binary Cross-Entropy, BCE)**
- **公式**: 对于二分类问题，单个样本的损失为：
	$$ L(y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})] $$
	其中 $y$ 是真实标签（0 或 1），$\hat{y}$ 是模型预测为正类的概率。
- **特点**: 衡量了两个概率分布之间的差异。当预测概率与真实标签相符时，损失很小；反之，损失很大。是[[逻辑回归\|逻辑回归]]和二分类[[神经网络\|神经网络]]的常用损失函数。
- **适用场景**: 二分类问题。

对于多分类问题，假设有 $K$ 个类别，真实标签 $y$ 是一个 one-hot 向量，预测概率 $\hat{y}$ 也是一个概率分布向量：

$$ L(y, \hat{y}) = -\sum_{k=1}^K y_k \log(\hat{y}_k) $$
- **特点**: 是二元交叉熵的推广。当模型预测的概率分布与真实标签的 one-hot 分布越接近，损失越小。通常与 [[Softmax函数\|Softmax函数]]一起使用作为多分类[[神经网络\|神经网络]]的输出层。
- **适用场景**: 多分类问题。


#### Focal Loss（焦点损失）
基础思想：为了解决类别极度不平衡问题，让模型关注难分类样本。
$$\text{FocalLoss}(p_t) = -\alpha(1 - p_t)^{\gamma} \log(p_t)$$
- $p_t$: 预测正确类别的概率。
- $\alpha$、$\gamma$: 调节参数。

特点：
- $\gamma > 0$ 抑制容易分类的样本。
- 强化难样本的学习。

应用场景：检测极小目标（如目标检测中的 RetinaNet）


**Hinge 损失 (Hinge Loss)**
- **公式**: $L(y, \hat{y}) = \max(0, 1 - y \cdot \hat{y})$
- **特点**: 主要用于[[支持向量机\|支持向量机]] (SVM)。它惩罚那些被错误分类的样本，以及那些虽然被正确分类但离决策边界太近的样本。
- **适用场景**: [[支持向量机\|支持向量机]]，以及一些最大间隔分类器。

### 排序 / 生成


### 损失函数的选择原则

-   **任务类型**: 回归问题通常选择 MSE、MAE 或 Huber；分类问题通常选择交叉熵。
-   **数据特性**: 如果数据中存在较多异常值，MAE 或 Huber 损失可能比 MSE 更合适。
-   **模型特性**: 某些模型（如 SVM）有其特定的损失函数。
-   **优化难度**: 损失函数的凸性、可导性会影响优化算法的选择和收敛性。