---
{"aliases":["Feature Engineering"],"dg-publish":true,"dg-path":"人工智能/机器学习/特征工程.md","tags":["ML"],"permalink":"/人工智能/机器学习/特征工程/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-08-28T21:53:13.510+08:00","updated":"2025-11-15T09:49:13.849+08:00"}
---


(terminology::**Feature Engineering**)
> 特征工程是利用领域知识和数据分析技能，从原始数据中提取、转换和选择特征，以最大化模型预测性能的过程。它是连接原始数据和机器学习模型的桥梁，是决定模型性能上限的关键步骤。



### 核心技术

特征工程的工作可以大致分为四大类：

#### 1. 特征创建 (Feature Creation)

从现有数据中手动构建新的特征。

- **组合特征**: 将多个特征进行数学运算（加、减、乘、除）形成新特征。*例如：从“商品价格”和“折扣”计算出“最终售价”。*
- **分解日期时间**: 从一个时间戳特征中，可以分解出“年份”、“月份”、“星期几”、“小时”等多个维度。
- **聚合统计特征**: 基于某个分组进行统计。*例如：计算一个用户“最近7天的平均消费额”、“历史总订单数”等。*

#### 2. 特征变换 (Feature Transformation)

改变现有特征的分布或尺度，使其更适合模型学习。

- **[[归一化函数\|归一化/标准化]]**: 将数值特征缩放到相似的尺度，避免某些特征因数值范围过大而主导模型训练。常见方法有Min-Max Scaling和Z-score Standardization。
- **离散化/分箱 (Discretization/Binning)**: 将连续特征（如年龄）转换为离散的类别特征（如“青年”、“中年”、“老年”）。这有助于捕捉非线性关系。
- **对数变换 (Log Transform)**: 对于长尾分布（Skewed Distribution）的数据，取对数可以使其更接近正态分布，提高模型的稳定性。

#### 3. 特征选择 (Feature Selection)

从所有特征中挑选出一个子集，以降低模型复杂度、减少过拟合风险和训练时间。

- **过滤法 (Filter Methods)**: 基于特征本身的统计属性（如相关系数、卡方检验、信息增益）进行评分和筛选，与后续要用的模型无关。
- **包装法 (Wrapper Methods)**: 将特征选择过程看作一个搜索问题，使用模型本身的性能（如准确率）作为评估标准来寻找最优特征子集。例如：递归特征消除 (RFE)。
- **嵌入法 (Embedded Methods)**: 在模型训练过程中自动进行特征选择。例如，L1正则化（如Lasso回归）可以使不重要特征的权重变为零。

#### 4. 特征提取 (Feature Extraction)

自动将高维特征空间映射到低维空间，创造出新的、更紧凑的特征表示。

- **[[主成分分析\|主成分分析]] (PCA)**: 一种经典的线性降维方法，寻找数据中方差最大的方向作为新的主成分。
- **[[自编码器\|自编码器]] (Auto-Encoder)**: 使用[[神经网络\|神经网络]]进行无监督学习，通过编码器将数据压缩，再通过解码器重构，其中间层的紧凑表示即为提取出的新特征。

### 自动化特征工程 (AutoFE)

随着[[AutoML\|AutoML]]技术的发展，自动化特征工程旨在自动发现和构建有用的特征，减少人工投入。但这通常需要巨大的计算资源，且生成特征的可解释性较差。

---

> [[机器学习\|机器学习]]
