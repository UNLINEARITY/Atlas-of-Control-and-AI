---
{"dg-publish":true,"dg-path":"计算机/机器学习/线性回归.md","permalink":"/计算机/机器学习/线性回归/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-08-29T21:21:47.821+08:00","updated":"2025-04-17T18:41:04.610+08:00"}
---


(terminology::**Linear Regression**) 
是一种通过属性的线性组合来进行预测的线性模型。其目的是找到一条直线或者一个平面或者更高维的超平面，**使得预测值与真实值之间的误差最小化**。

$$\begin{align}
\hat{y}=h(x)=w_{0}x_{0}+w_{1}x_{1}+\cdots + w_{n}x_{n}=w^{T}x
\end{align}$$
$$\begin{align}
J(w)= \dfrac{1}{2} \sum\limits_{i=1}^{m} (h(x)_{i}-y_{i})^{2}
\end{align}$$
$m$ 代表训练集中样本的数量     $n$ 代表特征的数量 
$x$ 代表特征/输入变量      $y$ 代表目标变量/输出变量
$(x, y)$ 代表训练集中的样本    $(x_{i},y_{i})$ 代表第 $i$ 个观察样本 
  $\hat{y}=h(x)$ 代表预测的值   $h$ 代表学习算法的解决方案或函数也称为假设（hypothesis）    
$J(w)$ 为损失函数，采用平方和损失，残差平方和
### 一、最小二乘法 LSM
不需要选择学习率,需要计算 $(X^{T}X)^{-1}$,只适用于线性模型 ，不适合逻辑回归模型等其他模型

算法流程：知 $h(x)$，寻找一组 $w(w_{0},w_{1},\cdots, w_{n})$ 使得残差平方和 $J(w)$ 最小
$$\begin{align}
\sum z_{i}^{2}=z^{T}z\quad \dfrac{\partial X^{T}X}{\partial X}=2X \quad  \dfrac{\partial AX}{\partial X}=A^{T}\quad \dfrac{\partial X^{T}AX}{\partial X}=(A+A^{T})X  
\end{align}$$

$$\begin{align}
\dfrac{\partial J(w)}{\partial w}= \dfrac{1}{2} \dfrac{\partial  }{\partial w}(Xw-Y)^{T}(Xw-Y)=\dfrac{1}{2} \dfrac{\partial }{\partial w}(w^{T}X^{T}Xw-2w^{T}X^{T}Y+Y^{T}Y)=\dfrac{1}{2}(2X^{T}Xw-2X^{T}Y+0)=0   
\end{align}$$

$X^{T}Xw-X^{T}Y=0\quad \Rightarrow \quad w=(X^{T}X)^{-1}X^{T}Y$
### 二、梯度下降 
需要选择学习率 $\alpha$，需要多次迭代，当特征数量𝑛大时也能较好适用，适用于各种类型的模型

- 批量梯度下降（Batch Gradient Descent,BGD）：梯度下降的每一步中，都用到了**所有**的训练样本
- 随机梯度下降（Stochastic Gradient Descent,SGD）：梯度下降的每一步中，用到**一个**样本，在每一次计算之后便更新参数，而不需要首先将所有的训练集求和
- 小批量梯度下降（Mini-Batch Gradient Descent,MBGD）：梯度下降的每一步中，用到了**一定批量**的训练样本
### 三、回归的评价指标

均方误差 MSE（Mean Square Error）：$\dfrac{1}{m} \sum\limits_{i=1}^{m}(y_{i}-\hat{y}_{i})^{2}$

均方根误差 RMSE (Root Mean Square Error): $\sqrt{ \dfrac{1}{m} \sum\limits_{i=1}^{m} (y_{i}-\hat{y}_{i})^{2} }$

平均绝对误差MAE（Mean Absolute Error): $\dfrac{1}{m} \sum\limits_{i=1}^{m}\left\lvert  y_{i}-\hat{y}_{i} \right\rvert$

RSquare $R^{2}= \dfrac{SSR}{SST}=1- \dfrac{SSE}{S S T}=1- \dfrac{MSE}{Var}$, 越接近于 1, 说明模型拟合得越好

$$\begin{align}
SS R= \sum\limits_{i=1}^{m}(\hat{y}_{i}-\bar{y})^{2}\quad  S S E=\sum\limits_{i=1}^{m}(\hat{y}_{i}-y)^{2}\quad  S S T=\sum\limits_{i=1}^{m}({y}_{i}-\bar{y})^{2}
\end{align}$$
