---
{"dg-publish":true,"dg-path":"人工智能/计算机视觉/Vision Transformer.md","permalink":"/人工智能/计算机视觉/Vision Transformer/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-04-28T22:38:34.840+08:00","updated":"2025-08-30T17:56:35.829+08:00"}
---

(terminology::**Vision Transformer**)  ViT
一种直接将 [[Transformer\|Transformer]] 架构应用到**图像分类任务**的方法，首次证明了在大规模数据集（比如ImageNet-21k）上，Transformer可以超越传统卷积神经网络（CNN）。
> **Google Brain**在2020年发布的论文《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》。

传统图像任务几乎都是基于[[CNN\|卷积神经网络]]
Vision Transformer通过简单地把图像分成小块并用Transformer处理，展现了强大的学习全局特征的能力，推动了计算机视觉从"卷积时代"走向了"自注意力时代"！


### 核心思想：图像即序列

传统的 [[卷积神经网络\|CNN]] 通过卷积操作，天然地利用了图像的局部性和平移不变性。而 ViT 则完全抛弃了卷积层，直接将图像“打碎”成一个个小块，然后像处理文本序列一样处理这些图像块。

-   **图像分块** : 将一张图像分割成固定大小的、不重叠的图像块（例如，16x16 像素）。
-   **序列化**: 将每个图像块展平（flatten）成一个向量，然后将这些向量组成一个序列。
-   **Transformer 处理**: 将这个图像块序列输入到标准的 [[Transformer\|Transformer]] 编码器中进行处理。


把**一张图像**当作**一串序列**来处理！
**步骤概览**：
1. **切图**：把图像切成固定大小的小块（patches）
2. **编码**：把每个小块线性映射成向量（类似词向量）
3. **加位置编码**：告诉模型每个块的位置
4. **送进Transformer编码器**：用标准的Transformer（多头自注意力+前馈网络）处理
5. **分类头**：用一个分类token输出最终类别

### 关键组件

ViT 的整体架构可以分为以下几个关键部分：

1.  **图像分块与线性嵌入 (Patch Embedding)**:
    -   将输入的图像 $X \in \mathbb{R}^{H \times W \times C}$（高、宽、通道数）分割成 $N$ 个固定大小的图像块 $X_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$，其中 $P$ 是图像块的边长。
    -   将每个图像块展平为一个向量，并通过一个线性投影层将其映射到 Transformer 的隐藏维度 $D$。

2.  **类别标记 (Class Token)**:
    -   在图像块序列的开头添加一个特殊的、可学习的**类别标记 (Class Token)**。这个标记的输出在 Transformer 编码器处理后，将作为整个图像的全局表示，用于最终的分类任务。

3.  **[[位置编码\|位置编码]] (Positional Encoding)**:
    -   由于[[自注意力机制\|自注意力机制]]不包含位置信息，为了保留图像块在原始图像中的空间位置信息，需要为每个图像块添加一个可学习的**位置编码**。这个位置编码与图像块的嵌入向量相加。

4.  **Transformer 编码器 (Transformer Encoder)**:
    -   由多个堆叠的 Transformer 编码器层组成。每个编码器层包含一个多头自注意力模块和一个前馈网络。
    -   它处理图像块序列，学习图像块之间的全局依赖关系，并生成上下文感知的图像块表示。



### 预训练与微调

ViT 通常需要在大规模数据集（如 JFT-300M，包含 3 亿张图像）上进行**预训练**，才能达到与 CNN 相当甚至超越 CNN 的性能。在预训练之后，再在特定任务的下游数据集上进行**微调 (Fine-tuning)**。

### 优缺点分析

| 优点 (Pros)                                  | 缺点 (Cons)                                                                  |
|----------------------------------------------|------------------------------------------------------------------------------|
| **全局上下文理解**：[[自注意力机制\|自注意力机制]]使其能够捕捉图像中任意位置的依赖关系。 | **计算资源需求大**：对于高分辨率图像，图像块数量多，导致计算量和内存消耗大。 |
| **可扩展性**：随着模型规模和数据量的增加，性能持续提升。 | **对数据量敏感**：通常需要大规模数据集进行预训练才能表现良好。             |
| **可解释性**：可以通过注意力权重可视化模型关注的区域。 | **局部信息处理**：ViT 没有像 CNN 那样内置的局部归纳偏置，可能需要更多数据来学习局部特征。 |

### 影响

ViT 的成功证明了 Transformer 架构在[[计算机视觉\|计算机视觉]]领域的巨大潜力，打破了 CNN 在该领域的长期主导地位。它启发了后续一系列基于 Transformer 的视觉模型，如 Swin Transformer、MAE 等，并推动了视觉领域“预训练+微调”范式的发展。