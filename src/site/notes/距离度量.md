---
{"aliases":["Distance Metric","Similarity Measure"],"dg-publish":true,"dg-path":"人工智能/机器学习/距离度量.md","tags":["ML","Mathematics"],"permalink":"/人工智能/机器学习/距离度量/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-12-24T00:36:26.056+08:00","updated":"2025-08-28T21:53:13.825+08:00"}
---


(terminology::**Distance Metric**)
> 距离度量（Distance Metric）或相似性度量（Similarity Measure）是[[机器学习\|机器学习]]和数据挖掘中用于**量化两个样本、特征或数据点之间相似性或差异性**的函数。它是许多算法（如[[K近邻\|K近邻]]、[[聚类\|聚类]]、[[支持向量机\|支持向量机]]）的核心组成部分，直接影响着模型的性能和结果。

### 核心作用：量化相似性

在机器学习中，我们经常需要判断“这个样本和那个样本有多像？”或者“这两个特征有多相关？”。距离度量提供了一个数学工具来回答这些问题。一个好的距离度量能够准确地反映数据点之间的内在关系。

### 距离度量的性质

一个函数 $d(x, y)$ 要被称为“距离度量”，通常需要满足以下四个基本性质：

1.  **非负性 (Non-negativity)**: $d(x, y) \ge 0$。距离总是非负的。
2.  **同一性 (Identity of indiscernibles)**: $d(x, y) = 0$ 当且仅当 $x = y$。只有当两个点是同一个点时，它们之间的距离才为零。
3.  **对称性 (Symmetry)**: $d(x, y) = d(y, x)$。从 $x$ 到 $y$ 的距离与从 $y$ 到 $x$ 的距离相等。
4.  **三角不等式 (Triangle Inequality)**: $d(x, z) \le d(x, y) + d(y, z)$。从 $x$ 到 $z$ 的直接距离不会超过从 $x$ 到 $y$ 再到 $z$ 的距离之和。

### 常见距离度量
#### 1. [[闵可夫斯基距离\|闵可夫斯基距离]] (Minkowski Distance)

这是最通用的一类距离度量，通过参数 $p$ 的选择，可以退化为多种常用距离。

$$ d_p(x, y) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p} $$

-   **[[欧氏距离\|欧氏距离]] (Euclidean Distance)**: 当 $p=2$ 时。最直观的直线距离。
-   **[[曼哈顿距离\|曼哈顿距离]] (Manhattan Distance)**: 当 $p=1$ 时。城市街区距离。
-   **切比雪夫距离 (Chebyshev Distance)**: 当 $p \to \infty$ 时。最大维度差。


[[范数\|范数]]
欧氏距离：在二维和三维空间中的欧氏距离就是两点之间的实际距离 
$$\begin{align}
d(x,y)=   \sqrt{\sum_{i}  (x_{i}-y_{i}) ^{2}}
\end{align}$$
曼哈顿距离/城市街区距离：
$$\begin{align}
d(x,y)= \sum_{i}  \left\lvert  x_{i}-y_{i} \right\rvert
\end{align}$$

切比雪夫距离：各坐标数值差绝对值的最大值
$$\begin{align}
d (x, y)= max \left\lvert  x_{i}-y_{i} \right\rvert 
\end{align}$$

闵可夫斯基距离：$p=1$ 时为曼哈顿距离, $p=2$ 时为欧氏距离，$p=\infty$ 时为切比雪夫距离
$$\begin{align}
d (x, y)=\left( \sum_{i} \left\lvert  x_{i}-y_{i} \right\rvert^{p} \right)^{ 1/p}
\end{align}$$




#### 2. 余弦相似度 (Cosine Similarity)

-   **原理**: 衡量两个向量在空间中的**方向相似性**，而不考虑它们的长度。值范围 $[-1, 1]$，越接近1表示越相似，越接近-1表示越不相似（方向相反），0表示正交（不相关）。
-   **公式**: 对于两个非零向量 $A$ 和 $B$：
    $$ \text{similarity} = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \sqrt{\sum_{i=1}^n B_i^2}} $$
-   **适用场景**: 文本分析（如文档相似度）、推荐系统，当向量的长度不重要，而方向更重要时。

#### 3. 汉明距离 (Hamming Distance)
**原理**: 用于衡量两个**等长字符串**（通常是二进制字符串）之间对应位置上不同字符的数量。它表示将一个字符串转换为另一个字符串所需的最小替换次数。

**公式**: 对于两个二进制字符串 $A$ 和 $B$：
$$ d_H(A, B) = \sum_{i=1}^n (A_i \ne B_i) $$

**适用场景**: 编码理论、信息论、DNA序列分析、类别变量的比较。



#### 4. 杰卡德相似系数 (Jaccard Similarity Coefficient)

-   **原理**: 用于衡量两个**集合**之间的相似性。它定义为两个集合交集的大小除以它们并集的大小。
-   **公式**: 对于两个集合 $A$ 和 $B$：
    $$ J(A, B) = \frac{|A \cap B|}{|A \cup B|} $$
-   **适用场景**: 文本相似度（如文档中词汇集合的相似度）、图像分割（衡量分割结果与真实标签的重叠度）、推荐系统。

### 选择原则

选择合适的距离度量是机器学习任务中的一个重要决策，需要考虑：

-   **数据类型**: 数值型、类别型、二进制、集合等。
-   **特征的尺度**: 如果特征尺度差异大，需要先进行[[归一化函数\|归一化/标准化]]。
-   **数据分布**: 某些距离度量对数据分布的假设不同。
-   **应用场景**: 不同的任务可能对距离的定义有不同的侧重。
-   **维度**: 高维数据可能面临维度灾难，某些距离度量在高维空间中表现不佳。

