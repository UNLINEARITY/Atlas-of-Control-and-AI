---
{"dg-publish":true,"dg-path":"人工智能/机器学习/归一化函数.md","permalink":"/人工智能/机器学习/归一化函数/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-04-28T23:49:32.000+08:00","updated":"2025-08-30T17:50:20.816+08:00"}
---

(terminology::**Normalization**)

把输入数据调整到特定范围或分布，以便后续处理更稳定、更高效。

是[[机器学习\|机器学习]]中常用的数据预处理技术，旨在将不同尺度、量纲的特征转换到统一的范围或分布。这对于许多机器学习算法的性能至关重要。
### 为什么需要归一化/标准化？

1.  **加速收敛**: 对于使用梯度下降等优化算法的模型（如神经网络、逻辑回归），如果特征的尺度差异很大，损失函数的等高线会非常扁平，导致梯度下降路径呈“之”字形，收敛速度慢。归一化/标准化可以使等高线更接近圆形，加速收敛。
2.  **避免特征主导**: 某些特征的数值范围可能远大于其他特征。如果不进行处理，这些大数值特征可能会在距离计算或权重更新中占据主导地位，使得模型无法公平地学习所有特征的重要性。
3.  **提高模型精度和稳定性**: 对于依赖距离度量的算法（如 K 近邻、支持向量机），归一化/标准化可以确保所有特征对距离的贡献是公平的。对于基于权重的模型，它有助于防止权重过大或过小，提高模型的稳定性。

### 一、输入归一化
#### Min-Max Scaling
$$\begin{align}
\hat{x}=\dfrac{x-x_{min}}{x_{max}-x_{min}}
\end{align}$$

- 图像像素值预处理（把0-255缩放到0-1）
- KNN等对数值尺度敏感的模型
#### Z-score Standardization
$$\begin{align}
\hat{x}=\dfrac{x-\mu}{\sigma}
\end{align}$$
标准化为均值 0 方差1
- 线性回归
- PCA降维
- 需要高斯分布假设的模型

#### 3. Decimal Scaling 归一化
-   **公式**: 通过移动小数点位置来归一化数据。
    $$ x' = \frac{x}{10^j} $$
    其中 $j$ 是使得 $|x'| < 1$ 的最小整数。
-   **特点**: 简单，但数据范围不固定，且不改变数据分布。
-   **适用场景**: 当数据范围差异不大，且对精度要求不高时。

#### 4. Max Abs 归一化
-   **公式**: 将数据缩放到 $[-1, 1]$ 之间，通过除以特征的最大绝对值。
    $$ x' = \frac{x}{\max(|x|)} $$
-   **特点**: 适用于稀疏数据，因为它不会改变数据的稀疏性。
-   **适用场景**: 稀疏特征，如文本数据。



### 二、激活归一化
> 深度学习中，隐藏层输出归一化，加速训练。
#### Batch Normalization (BN)
在每个 mini-batch 上, 计算均值 $\mu_{\text{batch}}$ 和方差 $\sigma_{\text{batch}}^{2}$, 然后归一化:
$$\hat{x} = \frac{x - \mu_{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^{2}} + \epsilon}$$
再引入两个可学习参数（仿射变换）:
$$
y = \gamma \hat{x} + \beta$$
应用:
- CNN、MLP、RNN（一定注意: RNN 里 BN 用得少）
- 深度残差网络 (ResNet)

#### Layer Normalization (LN)
在单一样本的所有神经元上计算均值方差：
$$\hat{x} = \frac{x - \mu_{layer}}{\sqrt{\sigma_{layer}^2 + \epsilon}}$$

应用：
- Transformer
- RNN（自然语言处理）
因为和 batch 大小无关，非常适合变长序列。
#### Instance Normalization (IN)
针对图像的一张一张样本，**每个通道单独归一化**。
应用：
- 风格迁移（Style Transfer）
- 图像生成领域（GAN）
#### Group Normalization (GN)
把通道分组，每组内部归一化。  介于BN（全局）和IN（局部）之间。
应用：小Batch Size场景（如目标检测）
### 三、输出归一化
#### Softmax
把任意实数向量映射成概率分布，核心用于**多分类输出**和**注意力机制权重归一化**，在神经网络训练中极其重要。

$$\begin{align}
y_{i}=softmax(a_{1},\cdots,a_{K})_{i}=\dfrac{e^{ a_{i}}}{\sum\limits_{j=1}^{K}e^{ a_{j} }}
\end{align}$$
$$\begin{align}
y_{i}=softmax(w_{i}^{T}x+w_{i_{0}})=   \dfrac{\exp\left[w_{i}^{T}x+w_{i_{0}}\right]}{\sum\limits_{j=1}^{K}\exp \left[w_{j}^{T}x+w_{j_{0}}\right]}
\end{align}$$

将一组输入值转换为概率分布。
最后一层通常使用 Softmax 函数，将输出转换为各个类别的概率。

非常重要！（尤其在神经网络训练时）
记 Softmax 输出为：
$$y_i = \text{Softmax}(z_i)$$
则对于输入 $z_k$，Softmax 的偏导数：
$$
\frac{\partial y_i}{\partial z_k} = \begin{cases}
y_i(1 - y_i), & \text{if } i = k \\
-y_iy_k, & \text{if } i \neq k
\end{cases}
$$
这可以写成一个 Jacobian 矩阵（雅可比矩阵）：
$$
\frac{\partial \mathbf{y}}{\partial \mathbf{z}} = \text{diag}(\mathbf{y}) - \mathbf{y}\mathbf{y}^T$$
解释：
- 对角线元素：自己对自己的导数 $y_i(1 - y_i)$。
- 非对角元素：互相之间的影响 $-y_iy_j$。
这也是为什么[[交叉熵\|交叉熵]]和 Softmax 通常合并计算梯度，能极大简化反向传播。
