---
{"dg-publish":true,"dg-path":"人工智能/机器学习/朴素贝叶斯分类.md","permalink":"/人工智能/机器学习/朴素贝叶斯分类/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-11-15T16:19:25.000+08:00","updated":"2025-08-30T17:26:46.000+08:00"}
---

(terminology::**Naive Bayes Classifier**)

典型的生成学习方法：训练数据学习[[二维分布函数\|联合概率分布]]，求得后验概率分布。（概率估计方法可以是极大似然估计或贝叶斯估计）


是一类基于[[贝叶斯定理\|贝叶斯定理]]的简单高效的分类算法。其核心建立在一个“朴素”的假设之上：**给定类别时，所有特征之间相互独立**。尽管这个假设在现实中几乎不成立，但它在许多应用中（尤其是文本分类) 取得了巨大成功。


### 贝叶斯方法
以[[贝叶斯公式\|贝叶斯公式]]为基础的分类方法
- **先验概率**  $P(Y)$：根据以往经验和分析得到的概率。在没有训练数据前假设 $Y$ 拥有的初始概率 
- **后验概率**  $P(Y\mid X)$：根据已经发生的事件来分析得到的概率。假设 $X$ 成立的情况下观察到 $Y$ 数据的概率，反映训练数据下 $Y$成立的置信度
- **联合概率**  $P(X,Y)\; P(XY)\; P(X \cap Y)$： 两个条件同时成立的概率

$$\begin{align}
P(B_{i}\mid A)&=\frac{P(AB_{i})}{P(A)} =\frac{P(B_{i})P(A\mid B_{i})}{\sum\limits_{i=1}^{n}P(A\mid B_{i})P(B_{i})}
\end{align}$$


### 一、核心思想：后验概率最大化

对于一个给定的样本 $X = (x_1, x_2, \dots, x_n)$，朴素贝叶斯分类的目标是找到使其[[后验概率\|后验概率]] $P(C_k | X)$ 最大的那个类别 $C_k$。

根据[[贝叶斯定理\|贝叶斯定理]]：
$$
P(C_k | X) = \frac{P(X | C_k) \cdot P(C_k)}{P(X)}
$$

-   $P(C_k)$: **先验概率**。类别 $C_k$ 本身出现的概率。
-   $P(X | C_k)$: **似然**。在类别 $C_k$ 中，观测到特征组合 $X$ 的概率。
-   $P(X)$: **证据**。特征组合 $X$ 出现的概率。

由于对于所有类别 $C_k$，$P(X)$ 的值都是相同的，因此在比较不同类别的后验概率时，可以忽略分母。我们的目标简化为最大化分子：
$$
\hat{C} = \arg\max_{C_k} P(X | C_k) \cdot P(C_k)
$$



### 朴素贝叶斯
基本假设是条件独立性，朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测

### 实际应用
拉普拉斯平滑的方法：我们为每个计数加1，因此它永远不会为零。为了平衡这一点，我们将可能单词的数量添加到除数中，因此计算结果永远不会大于1




> [!note]
>  



### 二、“朴素”的假设：特征条件独立

这里就是“朴素”一词的来源。我们假设所有特征 $x_i$ 在给定类别 $C_k$ 的情况下是条件独立的。这意味着：
$$
P(X | C_k) = P(x_1, x_2, \dots, x_n | C_k) = \prod_{i=1}^{n} P(x_i | C_k)
$$
这个假设极大地简化了似然的计算。将此代入分类准则，我们得到最终的朴素贝叶斯分类器公式：
$$
\hat{C} = \arg\max_{C_k} P(C_k) \prod_{i=1}^{n} P(x_i | C_k)
$$

> [!important] **对数技巧**
> 在实际计算中，多个小于 1 的概率相乘容易导致浮点数下溢。因此，通常会对上式取对数，将乘法变为加法，从而提高计算稳定性：
> $$
> \hat{C} = \arg\max_{C_k} \left( \log P(C_k) + \sum_{i=1}^{n} \log P(x_i | C_k) \right)
> $$

### 三、朴素贝叶斯的三种主要模型

根据特征数据的不同分布，朴素贝叶斯可以分为三种主要模型：

1.  **高斯朴素贝叶斯 (Gaussian Naive Bayes)**
    -   **适用场景**: 特征是连续值（如身高、体重）。
    -   **假设**: 假设每个特征在每个类别下都服从高斯分布。
    -   **似然计算**:
        $$
        P(x_i | C_k) = \frac{1}{\sqrt{2\pi\sigma_{k,i}^2}} \exp\left(-\frac{(x_i - \mu_{k,i})^2}{2\sigma_{k,i}^2}\right)
        $$
        其中 $\mu_{k,i}$ 和 $\sigma_{k,i}^2$ 是类别 $C_k$ 中特征 $x_i$ 的均值和方差。

2.  **多项式朴素贝叶斯 (Multinomial Naive Bayes)**
    -   **适用场景**: 特征是离散的计数值（如文本分类中单词的出现次数）。
    -   **假设**: 特征向量服从多项式分布。
    -   **似然计算**:
        $$
        P(x_i | C_k) = \frac{N_{k,i} + \alpha}{N_k + \alpha |V|}
        $$
        其中 $N_{k,i}$ 是类别 $C_k$ 中特征 $x_i$ 的总数，$N_k$ 是类别 $C_k$ 中所有特征的总数， $|V|$ 是词汇表大小，$\alpha$ 是平滑参数。

3.  **伯努利朴素贝叶斯 (Bernoulli Naive Bayes)**
    -   **适用场景**: 特征是二元的（0 或 1），表示某项是否存在（如文本分类中单词是否出现）。
    -   **假设**: 每个特征服从独立的伯努利分布。
    -   **似然计算**: 主要计算在类别 $C_k$ 中，特征 $x_i$ 出现（$P(x_i=1|C_k)$）和不出现（$P(x_i=0|C_k)$）的概率。

### 四、拉普拉斯平滑 (Laplace Smoothing)

为了防止因某个特征在训练集中未出现于某个类别中，导致其条件概率 $P(x_i|C_k)$ 为 0，从而使整个后验概率为 0 的问题，我们引入平滑技术。

Laplace Smoothing 是一种常用的平滑方法，它为所有计数值加一个小的平滑参数 $\alpha$（$\alpha=1$ 时称为拉普拉斯平滑，$\alpha<1$ 时称为 Lidstone 平滑）。

$$
P(x_i | C_k) = \frac{\text{count}(x_i, C_k) + \alpha}{\text{count}(C_k) + \alpha \cdot |V|}
$$
-   $|V|$: 特征词汇表的总大小。

### 五、优缺点分析

| 优点 (Pros) | 缺点 (Cons) |
| :--- | :--- |
| **简单高效**：实现简单，训练和预测速度快。 | **特征独立性假设过强**：与现实情况不符。 |
| **对小规模数据表现好**：在数据较少时依然有效。 | **对输入数据形式敏感**：需要为特征选择合适的模型。 |
| **适用于大规模数据集**：尤其在文本分类中表现出色。 | **分类精度可能受限**：在特征关联性强时表现不佳。 |

### 六、相关链接
- 贝叶斯定理
- 机器学习
- 文本分类
