---
{"aliases":["非线性最小二乘"],"dg-publish":true,"dg-path":"A1- 数学/未分类/最小二乘法.md","permalink":"/A1- 数学/未分类/最小二乘法/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-11-11T16:08:56.000+08:00","updated":"2025-06-19T01:12:32.507+08:00"}
---

(terminology::**Least Squares Method**)
> **最小化误差平方和** 来求解参数
> 最小二乘法是一切误差最小化问题的基础，是 SLAM、机器学习、数据拟合、参数估计的核心数学工具。

### 基础知识
目标函数：
$$
\min_{x} \sum_{i=1}^{n} \left( y_i - f(x, t_i) \right)^2 = \min_{x} \sum_{i=1}^n \| r_i(x) \|^2
$$
其中：
- $y_i$ ：第 $i$ 个观测值
- $f(x, t_i)$ ：预测值，取决于待求参数 $x$ 和自变量 $t_i$
- $r_i(x)$ ：第 $i$ 个残差（观测误差）


|类型|误差特性|解法|
|---|---|---|
|线性最小二乘|误差关于参数是线性的|解析解（直接计算）|
|非线性最小二乘|误差关于参数是非线性的|迭代法（数值优化）|


### 一、线性最小二乘法（解析解）
#### 微积分的角度

#### 线性代数的角度
[[向量投影\|向量投影]]
$\mathbf{b}=C+Dt$
对 $A\mathbf{x}=\mathbf{b}$，如果方程无解
则求解 $A^{T}A \hat{\mathbf{x}}=A^{T} \mathbf{b}$ 得到方程的最优近似解 $\hat{\mathbf{x}}$
使得 $E-\left\lvert  \left\lvert  A\mathbf{x}-\mathbf{b} \right\rvert \right\rvert^{2}$ 取得最小值



#### 概率论的角度
$$\begin{align}
e & =E[Y-(a+bX)]^{2} \\
\end{align}$$


$b_{0}=\dfrac{Cov(X,Y)}{D(X)}$ $a_{0}=E(Y)-b_{0}E(X)$

$$\begin{align}
min  E[Y-(a+bX)]^{2}&=E\left[Y-(a_{0}+b_{0}X)\right]^{2} =DY- \dfrac{Cov^{2}(X,Y)}{D(X)} 
\end{align}$$

### 二、非线性最小二乘法（迭代-数值优化）
非线性最小二乘法 **无法直接求解析解，必须使用数值迭代法**。

1. 初始化参数 $x_0$
2. 计算当前残差 $r(x)$ 和雅可比矩阵 $J(x)$
3. 构造线性方程，计算步长 $\Delta x$
4. 判断是否收敛（如步长足够小，残差变化足够小）
5. 更新参数 $x \leftarrow x + \Delta x$
6. 重复步骤 2 ~ 5 直到收敛

#### 一阶泰勒展开（线性化）

$$
r(x + \Delta x) \approx r(x) + J(x) \Delta x
$$

其中：$J(x)$ ：[[雅可比矩阵\|雅可比矩阵]]
$$
J(x) = \frac{\partial r(x)}{\partial x}
$$

#### 高斯-牛顿法（Gauss-Newton）
通过线性化，将优化转化为如下问题：
$$
\min_{\Delta x} \| r(x) + J(x) \Delta x \|^2
$$

解得：
$$
(J^T J) \Delta x = -J^T r(x)
$$

这是一个 **线性方程组** ，解出 $\Delta x$ ，然后更新：
$$
x_{k+1} = x_k + \Delta x
$$

不断迭代，直到收敛。

####  Levenberg-Marquardt (LM) 法（更稳定）
高斯-牛顿法可能遇到：
- 步长过大导致发散
- $J^T J$ 奇异无法求逆
LM 法引入阻尼因子 $\lambda$ ，将更新方程改为：
$$
(J^T J + \lambda I) \Delta x = -J^T r(x)
$$

特点：
- 类似于梯度下降法，步长更安全
- 动态调整 $\lambda$ 保证收敛速度与稳定性

#### Dogleg 法（结合 GN 与梯度下降）
适用于稀疏大规模问题，结合两种步长，提升收敛效率。

### 三、实际应用

非线性最小二乘法库： [[g2o\|g2o]]  [[Ceres Solver\|Ceres Solver]]
$$
\min_{\{x_i\}} \sum_{(i,j) \in C} \| z_{ij} - h(x_i, x_j) \|^2_{\Omega_{ij}}
$$

其中：
- $x_i$ ：机器人位姿节点
- $z_{ij}$ ：观测数据
- $h(x_i, x_j)$ ：通过运动模型计算的预测观测
- $\Omega_{ij}$ ：信息矩阵

核心任务：利用非线性最小二乘法优化整张图，找到最一致的位姿和地图。

