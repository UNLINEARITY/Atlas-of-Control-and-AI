---
{"aliases":["Gradient Descent"],"dg-publish":true,"dg-path":"人工智能/机器学习/梯度下降.md","tags":["ML","Mathematics"],"permalink":"/人工智能/机器学习/梯度下降/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-08-28T21:53:13.320+08:00","updated":"2025-09-08T13:21:40.000+08:00"}
---


(terminology::**Gradient Descent**)
> 梯度下降是[[机器学习\|机器学习]]和[[深度学习\|深度学习]]中最常用、最基础的**优化算法**。其核心思想是，通过沿着[[损失函数\|损失函数]]（或目标函数）梯度的**反方向**，以迭代的方式调整模型参数，从而逐步逼近损失函数的**局部最小值**（或全局最小值）。

### 核心思想：下山寻谷

想象你身处一座大山（损失函数曲面）的某个位置，目标是找到山谷（损失函数的最小值）。由于你无法一眼望到整个山谷，最直观的方法就是：

1.  **确定当前位置的坡度 (梯度)**: 找到当前位置最陡峭的下降方向。
2.  **迈出一步**: 沿着这个最陡峭的下降方向走一小步。
3.  **重复**: 不断重复上述过程，直到到达山谷底部。

这个“坡度”就是损失函数对模型参数的**梯度 (Gradient)**，它是一个向量，指向函数值增长最快的方向。因此，我们沿着梯度的反方向移动，就能使函数值减小。

### 数学公式

对于一个损失函数 $J(\theta)$，其中 $\theta$ 是模型的参数（如权重和偏置），梯度下降的参数更新规则为：

$$ \theta_{new} = \theta_{old} - \alpha \nabla J(\theta_{old}) $$

其中：
- $\theta_{new}$ 是更新后的参数。
- $\theta_{old}$ 是当前参数。
- $\alpha$ 是**学习率 (Learning Rate)**，一个正的超参数，控制着每次更新的步长。
- $\nabla J(\theta_{old})$ 是损失函数 $J$ 在当前参数 $\theta_{old}$ 处的梯度。

### 学习率 (Learning Rate)

学习率 $\alpha$ 是梯度下降中最重要的超参数之一。它的选择直接影响着算法的收敛速度和能否找到最优解。

-   **学习率过大**: 可能导致算法在最小值附近来回震荡，甚至发散，无法收敛。
-   **学习率过小**: 导致算法收敛速度过慢，需要大量的迭代才能达到最小值。

在实践中，通常会采用**学习率调度 (Learning Rate Scheduling)** 策略，在训练过程中动态调整学习率，例如逐渐减小学习率。

### 主要变种

梯度下降有多种变种，主要区别在于每次更新参数时使用的数据量：

#### 1. 批量梯度下降 (Batch Gradient Descent, BGD)

-   **特点**: 每次更新参数时，使用**所有**训练样本来计算梯度。
-   **优点**: 每次更新都沿着全局最优方向，收敛路径平稳，最终能收敛到局部最小值。
-   **缺点**: 对于大规模数据集，计算成本高，训练速度慢，内存消耗大。

#### 2. 随机梯度下降 (Stochastic Gradient Descent, SGD)

-   **特点**: 每次更新参数时，只随机选择**一个**训练样本来计算梯度。
-   **优点**: 计算速度快，内存消耗小，可以处理大规模数据集。由于每次更新的随机性，有助于跳出局部最小值。
-   **缺点**: 梯度更新方向具有随机性，导致损失函数震荡剧烈，收敛路径不稳定，可能无法精确收敛到最小值。

#### 3. 小批量梯度下降 (Mini-Batch Gradient Descent)

-   **特点**: 每次更新参数时，使用**一小批 (Mini-Batch)** 训练样本来计算梯度。这是实践中最常用的方法。
-   **优点**: 兼顾了BGD的稳定性和SGD的速度。通过批量计算，可以利用现代硬件的并行计算能力。
-   **缺点**: 批量大小的选择需要经验。

#### 4. 自适应学习率优化器

为了进一步提高收敛速度和稳定性，研究者们开发了许多更高级的优化器，它们能够**自适应地调整学习率**：

-   **Adagrad**: 根据参数的历史梯度平方和来调整学习率，对不频繁的参数给予更大的学习率。
-   **RMSprop**: 解决了Adagrad学习率下降过快的问题，通过指数加权平均来计算历史梯度平方和。
-   **Adam (Adaptive Moment Estimation)**: 结合了Adagrad和RMSprop的优点，同时考虑了梯度的**一阶矩（均值）**和**二阶矩（非中心方差）**，是目前最流行、最常用的优化器之一。

### 挑战

-   **局部最小值与鞍点**: 梯度下降可能陷入局部最小值或鞍点，无法达到全局最优。
-   **收敛速度**: 学习率的选择和损失函数曲面的形状会影响收敛速度。

---

> [[机器学习\|机器学习]]
> [[神经网络\|神经网络]]
