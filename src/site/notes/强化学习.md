---
{"dg-publish":true,"dg-path":"人工智能/机器学习/强化学习.md","permalink":"/人工智能/机器学习/强化学习/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-04-02T00:29:14.000+08:00","updated":"2025-05-23T17:26:37.000+08:00"}
---


(terminology::Reinforcement Learning)  RL 
[[机器学习\|机器学习]]的一个重要分支，通过让智能体（Agent）在环境中（Environment）进行试错（Trial and Error），并根据反馈来学习最优行为策略，以最大化累积奖励（Cumulative Reward）。
**强化学习是通过与环境交互、积累经验、优化策略，以最大化累计奖励的过程**。
### 1. 强化学习的基本概念
强化学习的过程可以用一个五元组来描述：
$$\begin{align}
(\mathcal{S},\mathcal{A} , \mathcal{P},\mathcal{R},\gamma   )
\end{align}$$

智能体的目标是学会一个策略（policy），使得**累计期望回报最大**：
$$
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots \; {\color{red}\Rightarrow} \; \max \mathbb{E}\left[G_{t}\right]
$$

| 符号            | 含义     |                                                 |
| ------------- | ------ | ----------------------------------------------- |
| $\mathcal{S}$ | 状态空间   | 环境当前的观测                                         |
| $\mathcal{A}$ | 动作空间   | 智能体能在当前状态下采取的行为                                 |
| $\mathcal{P}$ | 状态转移函数 | 给定状态和动作后转移到下一个状态的概率，用于刻画环境的动态特性。                |
| $\mathcal{R}$ | 奖励函数   | 衡量智能体动作好坏的指标，即它能从环境中获得多少即时奖励。                   |
| $\gamma$      | 折扣因子   | 衡量未来奖励的重要性，取值在 $[0,1]$ 之间，$\gamma$ 越大，未来的奖励越重要。 |

### 关键概念和方法



### 2. 强化学习的工作原理
强化学习的核心是让智能体通过与环境的交互来学习最优策略。其基本流程如下：
1. 初始化：智能体处于初始状态。
2. 选择动作：根据当前策略选择一个动作。
3. 执行动作：智能体执行动作，环境根据动作给出新的状态和奖励。
4. 更新策略：根据奖励和新的状态，智能体更新策略，以提高未来获得的累积奖励。
5. 重复：重复上述过程，直到达到目标状态或满足终止条件。

[[马尔可夫决策过程\|马尔可夫决策过程]]

### 3. 强化学习的主要算法
强化学习的算法可以分为两大类：基于价值的算法和基于策略的算法。

基于价值的算法（Value-Based Methods）
基于策略的算法（Policy-Based Methods）
结合价值与策略的算法  Actor-Critic


| 算法类别               | 代表算法                              | 特点                 |
| ------------------ | --------------------------------- | ------------------ |
| 基于值（Value-based）   | [[Q-Learning\|Q-Learning]]<br>[[DQN\|DQN]]         | 学习Q函数，动作通过Q值最大化选择  |
| 基于策略（Policy-based） | [[REINFORCE\|REINFORCE]]<br>[[PPO\|PPO]]          | 直接学习策略分布，适合高维动作空间  |
| Actor-Critic方法     | A2C<br>A3C<br>[[DDPG\|DDPG]]<br>[[SAC\|SAC]] | 同时学习策略和值函数，收敛更快更稳定 |
