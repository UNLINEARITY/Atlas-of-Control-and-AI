---
{"dg-publish":true,"dg-path":"A1- 数学/4. 线性代数/矩阵求导.md","permalink":"/A1- 数学/4. 线性代数/矩阵求导/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-05-21T15:20:28.562+08:00","updated":"2025-04-25T12:33:23.787+08:00"}
---



**Matrix Calculus**

[[矩阵\|矩阵]]的[[导数\|导数]]运算
### 一、标量方程对向量的导数

分子： $f(\vec{y})$ 为 $1\times1$ 的标量
分母： $\vec{y}$    为 $n\times1$ 的[[向量\|向量]]

#### 分母布局
与分母的函数相同

$$\begin{align}
\dfrac{\partial f(\vec{y})}{\partial\vec{y}}=\begin{pmatrix}
\dfrac{\partial f(\vec{y})}{\partial y_{1}}\\
\dfrac{\partial f(\vec{y})}{\partial y_{2}} \\
\vdots \\
\dfrac{\partial f(\vec{y})}{\partial y_{n}}
\end{pmatrix} 
\end{align}$$

#### 分子布局
与分子的行数相同

$$\begin{align}
\dfrac{\partial f(\vec{y})}{\partial\vec{y}}=\begin{pmatrix}
\dfrac{\partial f(\vec{y})}{\partial y_{1}},
\dfrac{\partial f(\vec{y})}{\partial y_{2}} ,\dots ,
\dfrac{\partial f(\vec{y})}{\partial y_{n}}
\end{pmatrix} 
\end{align}$$


### 二、向量方程对向量的导数

$\vec{f} (\vec{y})=\begin{pmatrix} f_{1}(\vec{y})\\ f_{2}(\vec{y})\\ \vdots\\f_{n}(\vec{y})\end{pmatrix}_{(n\times1)}$     $\quad\quad \vec{y}=\begin{pmatrix}y_{1}\\y_{2}\\\vdots\\y_{m}\end{pmatrix}_{(m\times 1)}$

#### 分母布局

$$\begin{align}
\dfrac{\partial \vec{f} (\vec{y})}{\partial \vec{y}} =\begin{pmatrix}
\dfrac{\partial \vec{f}(\vec{y})}{\partial y_{1}}\\
\dfrac{\partial \vec{f}(\vec{y})}{\partial y_{2}} \\
\vdots \\
\dfrac{\partial \vec{f}(\vec{y})}{\partial y_{m}}
\end{pmatrix}_{(m\times1)} = \begin{pmatrix}
 & \dfrac{\partial f_{1}(\vec{y})}{\partial y_{1}}
 & \dfrac{\partial f_{2}(\vec{y})}{\partial y_{1}}  & \dots 
 & \dfrac{\partial f_{n}(\vec{y})}{\partial y_{1}} \\
 & \dfrac{\partial f_{1}(\vec{y})}{\partial y_{2}}
 & \dfrac{\partial f_{2}(\vec{y})}{\partial y_{2}}  & \dots 
 & \dfrac{\partial f_{n}(\vec{y})}{\partial y_{2}} \\
 & \dots
 & \dots  & \dots 
 & \dots \\
 & \dfrac{\partial f_{1}(\vec{y})}{\partial y_{m}}
 & \dfrac{\partial f_{2}(\vec{y})}{\partial y_{m}}  & \dots 
 & \dfrac{\partial f_{n}(\vec{y})}{\partial y_{m}}
\end{pmatrix}_{(m\times n)}
\end{align}$$


 
### 结论
在分母布局的条件下：
$$\begin{align}
\dfrac{\partial A \vec{y}}{\partial  \vec{y}}&=A^{T}
\end{align}$$

[[二次型\|二次型]]的求导
$$\begin{align}
\dfrac{\partial  \vec{y}^{T}A \vec{y}}{\partial  \vec{y}}=A \vec{y}+ A^{T}  \vec{y} 
\end{align}$$

如果 $A$ 为对称阵，则有 $\dfrac{\partial  \vec{y}^{T}A \vec{y}}{\partial  \vec{y}}= 2A \vec{y}$


### 链式法则
[[导数#复合函数的求导法则\|标量的链式求导]]
注意矩阵不满足乘法交换律
使用分母布局：
$$\begin{align}
\dfrac{\partial J}{\partial  \vec{u}}=\dfrac{\partial  \vec{y}}{\partial \vec{u}}  \dfrac{\partial J}{\partial  \vec{y}} 
\end{align}$$

离散状态空间方程： $\vec{x}_{[k+1]}=A \vec{x}_{[k]}+B \vec{u}_{[k]}$
代价函数： $J=\vec{x}_{[k+1]}^{T}\vec{x}_{[k+1]}$
$\dfrac{\partial J}{\partial  \vec{u}}=\dfrac{\partial  \vec{x}_{[k+1]}}{\partial  \vec{u}_{[k]}}\dfrac{\partial J}{\partial  \vec{x}_{[k+1]}}=B^{T}\cdot 2\vec{x}_{[k+1]}$


### 实际应用
[[线性回归\|线性回归]]


$$\begin{align}
\hat{z}&=y_{1}+y_{2}x \\
J&=\sum\limits_{i=1}^{n} \left[z_{i}-(y_{1}+y_{2}x_{i})\right]^{2}
\end{align}$$
使用矩阵求解


$\vec{z}=\begin{pmatrix}z_{1}\\ z_{2}\\ \vdots \\ z_{n}\end{pmatrix}\quad x=\begin{pmatrix}  1& x_{1}\\ 1&x_{2} \\ \vdots&\vdots\\ 1&x_{n} \end{pmatrix}\quad y=\begin{pmatrix}y_{1}\\y_{2}\end{pmatrix}\quad  \vec{\hat{z}}=\vec{x} \vec{y}=\begin{pmatrix}  y_{1}+y_{2}x_{1}\\  y_{1}+y_{2}x_{2}\\ \vdots\\  y_{1}+y_{2}x_{n} \end{pmatrix}$


$$\begin{align}
\vec{z}-\vec{\hat{z}}&=\begin{pmatrix}
z_{1}-(y_{1}+y_{2}x_{1}) \\
z_{2}-( y_{1}+y_{2}x_{2})\\ \vdots\\  z_{n}-(y_{1}+y_{2}x_{n} )
\end{pmatrix} \\
J&=[\vec{z}-\vec{\hat{z}}]^{T}[\vec{z}-\vec{\hat{z}}] \\
&=[\vec{z}^{T}-\vec{y}^{T}\vec{x}^{T}][\vec{z}-\vec{x}\vec{y}]  \\
&=\vec{z}^{T}\vec{z}-\vec{y}^{T}\vec{x}^{T}\vec{z}-\vec{z}^{T}\vec{x}\vec{y}+\vec{y}^{T}\vec{x}^{T}\vec{x}\vec{y} \\
&=\vec{z}^{T}\vec{z}-2\vec{z}^{T}\vec{x}\vec{y}+\vec{y}^{T}\vec{x}^{T}\vec{x}\vec{y}
\end{align}$$

$$\begin{align}
&\dfrac{\partial J}{\partial  \vec{y}}=-2(\vec{z}^{T}\vec{x})+2 \vec{x}^{T}\vec{x}\vec{y}= 0  \\
&\Rightarrow \vec{y}=(\vec{x}^{T}\vec{x})^{-1} \vec{x}^{T}\vec{z}
\end{align}$$

如果没有解析解，使用机器学习的梯度下降

定义初始值 $\vec{y}^{*}$
循环迭代  $\vec{y}^{*}=\vec{y}^{*}-\alpha \nabla$
- 梯度： $\nabla=\dfrac{\partial J}{\partial  \vec{y}}$     
- 学习率： $\alpha=\begin{pmatrix} \alpha_{1} & 0\\ 0& \alpha_{2}\end{pmatrix}$
	注意如果没有[[数据处理#归一化\|归一化处理]]，学习率的选取要结合未知向量各值的数量级
	归一化处理后：各个未知数用同一个学习率，使用标量 $\alpha$ 即可




