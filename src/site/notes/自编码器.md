---
{"aliases":["AE","Autoencoder"],"dg-publish":true,"dg-path":"人工智能/深度学习/自编码器.md","tags":["DL"],"permalink":"/人工智能/深度学习/自编码器/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-08-28T21:53:13.000+08:00","updated":"2025-09-05T12:41:42.000+08:00"}
---


(terminology::**Autoencoder**)
> 自编码器是一种无监督的[[神经网络\|神经网络]]，其核心目标是学习输入数据的**有效表示 (Representation)**，通常用于**数据降维**或**特征提取**。它通过训练网络去**重构其自身的输入**来实现这一目标，即让输出尽可能地与输入相同。

### 核心思想：编码与解码

自编码器的思想非常精巧：它强迫数据通过一个“信息瓶颈”，然后再尝试将其恢复。如果网络能够成功地从这个瓶颈中重构出原始数据，那么这个瓶颈本身就包含了对原始数据最重要、最核心信息的**压缩表示**。

### 结构组成

一个标准的自编码器由两个主要部分组成，形成一个对称的、沙漏状的结构：

1.  **编码器 (Encoder)**: 负责将高维的输入数据 $x$ 逐步压缩，并将其映射到一个低维的**潜在空间 (Latent Space)**，得到一个**潜在表示**或**编码** $z$。这个过程可以看作是数据的“压缩”。

2.  **瓶颈 (Bottleneck)**: 网络中间最窄的一层，即潜在表示 $z$ 所在的位置。它是对输入数据最紧凑的表示。

3.  **解码器 (Decoder)**: 负责接收潜在表示 $z$，并尝试将其“解压”，重构出原始的输入数据，得到输出 $\hat{x}$。

### 训练目标：最小化重构误差

自编码器是一种**自监督学习**，因为它不需要外部标签。它的“标签”就是输入数据本身。训练的目标是最小化**重构误差 (Reconstruction Error)**，即输入 $x$ 和解码器输出 $\hat{x}$ 之间的差异。

常用的[[损失函数\|损失函数]]包括：
- **均方误差 (Mean Squared Error, MSE)**: 适用于图像等连续值数据。
- **交叉熵 (Cross-Entropy)**: 适用于二值化数据。

$$ L(x, \hat{x}) = \| x - \hat{x} \|^2 $$

### 主要变体与应用

基础的自编码器可能会学习到一个“恒等函数”，即简单地复制输入到输出，而没有学到有用的特征。为了迫使网络学习到更有意义的表示，研究者们提出了多种变体：

- **稀疏自编码器 (Sparse Autoencoder)**: 在损失函数中加入一个正则化项，限制隐藏层中被激活神经元的数量，从而学习到更稀疏的表示。

- **降噪自编码器 (Denoising Autoencoder)**: 在训练时，故意向输入数据 $x$ 中加入一些随机噪声，得到一个损坏的版本 $\tilde{x}$。然后，训练网络从这个损坏的输入 $\tilde{x}$ 中重构出**原始、干净的**数据 $x$。这迫使网络去学习数据中更鲁棒、更稳定的特征，而不仅仅是复制输入。

- **变分自编码器 (Variational Autoencoder, VAE)**: 这是自编码器思想的一个重大扩展，使其成为一个强大的**深度生成模型**。VAE的编码器输出的不是一个单一的编码向量，而是一个**概率分布**（通常是高斯分布的均值和方差）。解码器则从这个分布中采样来进行重构。通过对潜在空间施加这种概率约束，VAE可以生成平滑、连续的潜在空间，从而能够创造出全新的、与训练数据相似但又不完全相同的新样本。

### 应用领域

- **数据降维与可视化**: 学习到的低维编码可用于数据可视化（如t-SNE的预处理）或作为其他[[机器学习\|机器学习]]模型的输入特征。
- **特征提取**: 无监督地从数据中学习有用的特征。
- **异常检测**: 如果模型在正常数据上训练，那么当输入一个异常数据时，其重构误差会非常大，从而可以被识别出来。
- **数据生成**: 特别是VAE，广泛用于图像、音乐等内容的生成。

