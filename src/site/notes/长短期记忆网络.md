---
{"aliases":["LSTM","Long Short-Term Memory"],"dg-publish":true,"dg-path":"人工智能/深度学习/长短期记忆网络.md","tags":["DL"],"permalink":"/人工智能/深度学习/长短期记忆网络/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-08-28T21:53:13.850+08:00","updated":"2025-08-28T21:53:14.000+08:00"}
---


(terminology::**Long Short-Term Memory**)
> 长短期记忆网络（LSTM）是[[循环神经网络\|循环神经网络]]（RNN）的一种特殊变体。它被精心设计用来解决标准RNN在处理长序列时遇到的**长程依赖 (Long-Term Dependency)** 问题。LSTM通过引入一个**细胞状态 (Cell State)** 和三个精巧的**门控机制 (Gating Mechanisms)**，能够有效地学习、记忆和遗忘序列中的长期信息，是深度学习在序列建模领域的里程碑式成就。

### 核心动机：克服梯度消失

标准[[循环神经网络\|循环神经网络]]在理论上可以连接过去的信息到当前任务，但在实践中，如果序列过长，在训练时会遭遇**梯度消失**问题。这意味着来自早期时间步的、用于更新网络权重的梯度信号，在通过许多时间步反向传播后会变得极其微弱，导致网络无法学习到长期依赖关系。LSTM的结构正是为了克服这一缺陷而设计的。

### LSTM的核心：细胞状态与门

LSTM的关键在于其单元（Cell）内部的设计。除了像普通RNN一样传递隐藏状态（$h_t$）外，LSTM还引入了一个贯穿整个处理链的**细胞状态（$C_t$）**，可以看作是一条“记忆传送带”。理论上，信息可以在这条传送带上一直流动而不发生改变。而对这条传送带上信息的增删改查，则是由三个“门”来控制的。

每个“门”都是一个小型的[[神经网络\|神经网络]]层（通常是一个Sigmoid激活函数）和一个按元素相乘的操作。Sigmoid层的输出介于0和1之间，表示允许多少信息通过（0代表完全不允许，1代表完全允许）。

#### 1. 遗忘门 (Forget Gate)

> **作用**: 决定从细胞状态中**丢弃**什么信息。

它查看上一个时间步的隐藏状态 $h_{t-1}$ 和当前输入 $x_t$，然后为上一个细胞状态 $C_{t-1}$ 中的每个数字输出一个0到1之间的值。1表示“完全保留”，0表示“完全遗忘”。

$$ f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) $$

#### 2. 输入门 (Input Gate)

> **作用**: 决定让什么**新信息**存入细胞状态。

这个过程分两步：
1.  一个Sigmoid层（输入门）决定哪些值需要更新。
2.  一个tanh层创建一个新的候选值向量 $\tilde{C}_t$。

$$ i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) $$
$$ \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) $$

然后，结合遗忘门和输入门的结果，来更新细胞状态：

$$ C_t = f_t * C_{t-1} + i_t * \tilde{C}_t $$

#### 3. 输出门 (Output Gate)

> **作用**: 决定要从细胞状态中**输出**什么信息。

1.  一个Sigmoid层决定细胞状态的哪些部分将被输出。
2.  将细胞状态通过一个tanh层（将其值缩放到-1到1之间），然后与Sigmoid门的输出相乘。

这样，模型就只输出它认为相关的部分作为当前时间步的隐藏状态 $h_t$。

$$ o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) $$
$$ h_t = o_t * \tanh(C_t) $$

### GRU：一个流行的变体

**门控循环单元 (Gated Recurrent Unit, GRU)** 是LSTM的一个简化版本。它将遗忘门和输入门合并为单一的“更新门”，并混合了细胞状态和隐藏状态。GRU的参数更少，计算上更高效，在许多任务中表现与LSTM相当，因此非常受欢迎。

### 地位的演变

在[[Transformer\|Transformer]]架构出现之前，LSTM及其变体是处理序列数据的绝对王者，在机器翻译、文本生成等领域取得了巨大成功。然而，由于其固有的序列计算依赖性，使其难以并行化。如今，在许多大规模自然语言处理任务中，基于注意力机制的[[Transformer\|Transformer]]已经取代了LSTM的主导地位。尽管如此，LSTM因其高效性和在某些特定场景下的优异表现，仍在许多应用中占有一席之地。

---

> [[循环神经网络\|循环神经网络]]
