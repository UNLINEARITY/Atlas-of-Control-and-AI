---
{"aliases":["RNN","Recurrent Neural Network"],"dg-publish":true,"dg-path":"人工智能/深度学习/循环神经网络.md","tags":["DL"],"permalink":"/人工智能/深度学习/循环神经网络/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-08-28T21:53:13.150+08:00","updated":"2025-08-28T21:53:14.000+08:00"}
---


(terminology::**Recurrent Neural Network**)
> 循环神经网络（RNN）是一类专门用于处理**序列数据**的[[神经网络\|神经网络]]。与一次只处理一个固定输入的前馈神经网络（如CNN）不同，RNN通过其内部的**循环结构**，能够将历史信息编码并传递到当前时间步，从而有效捕捉数据中的**时间依赖关系**。

### 核心思想：记忆与循环

人类在理解一句话时，会根据前面听到的词来理解当前词的含义。RNN正是模仿了这种行为。它的核心思想是，在每个时间步，网络不仅接收当前的输入，还接收来自**上一个时间步的隐藏状态 (Hidden State)**。这个隐藏状态可以被看作是网络对过去所有信息的“记忆”或“摘要”。

这种将输出再次作为输入的循环结构，使得RNN能够处理任意长度的序列。

### 结构与工作原理

一个RNN单元可以被看作是一个“模块”，在处理序列时，这个模块被反复使用。

- **折叠视图 (Folded View)**: 在图中，RNN单元会有一个指向自身的循环箭头，表示其输出会作为下一次计算的输入。
- **展开视图 (Unfolded View)**: 将循环按时间步展开，可以更清晰地看到信息在序列中的流动。在时间步 $t$，RNN单元接收当前输入 $x_t$ 和上一个时间步的隐藏状态 $h_{t-1}$，然后计算出当前时间步的输出 $y_t$ 和新的隐藏状态 $h_t$。这个新的 $h_t$ 将被传递给时间步 $t+1$。

$$ h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h) $$
$$ y_t = g(W_{hy}h_t + b_y) $$

其中，$W$ 和 $b$ 是网络需要学习的权重和偏置，$f$ 和 $g$ 是[[激活函数\|激活函数]]。

### 挑战：长程依赖问题 (Long-Term Dependencies)

理论上，RNN可以处理任意长度的序列。但在实践中，标准RNN（也称“香草RNN”）难以学习到序列中相距较远元素之间的依赖关系。这主要是由**梯度消失/爆炸 (Vanishing/Exploding Gradients)** 问题引起的。

在通过时间反向传播 (BPTT) 训练RNN时，梯度需要在时间步之间连乘。如果梯度值持续小于1，经过多次连乘后会迅速趋近于零（**梯度消失**），导致网络无法学习到早期时间步的信息；反之，如果梯度值持续大于1，则会指数级增长导致数值不稳定（**梯度爆炸**）。

### 解决方案：门控RNN (Gated RNNs)

为了解决长程依赖问题，研究者们设计了更复杂的RNN单元，引入了“门控机制”来有选择地让信息通过，从而更好地控制记忆的更新、遗忘和输出。

- **[[长短期记忆网络\|长短期记忆网络]] (LSTM)**: 最著名、最成功的RNN变体。LSTM引入了三个精巧的“门”：
    - **遗忘门 (Forget Gate)**: 决定应该从细胞状态中丢弃什么信息。
    - **输入门 (Input Gate)**: 决定什么样的新信息可以被存放到细胞状态中。
    - **输出门 (Output Gate)**: 决定要从细胞状态中输出什么信息。

- **门控循环单元 (Gated Recurrent Unit, GRU)**: LSTM的一个简化版本，将遗忘门和输入门合并为一个“更新门”，并混合了细胞状态和隐藏状态。GRU的参数更少，计算效率更高，在许多任务上能达到与LSTM相当的性能。

### 应用领域

RNN及其变体在许多序列建模任务中都取得了巨大成功：

- **自然语言处理 (NLP)**: 语言建模、机器翻译、文本生成、情感分析。
- **语音识别**: 将声学信号转换为文本。
- **时间序列分析**: 股票价格预测、天气预报。
- **视频分析**: 理解视频中帧与帧之间的动态关系。

### 局限性

尽管非常强大，但RNN及其变体存在一个固有瓶颈：**计算的序列依赖性**。当前时间步的计算必须等待前一个时间步完成，这使得RNN难以进行大规模的并行计算。这一局限性促使了[[Transformer\|Transformer]]等完全基于注意力机制的并行化架构的诞生，并最终在许多领域超越了RNN。

---

> [[深度学习\|深度学习]]
