---
{"alias":["Softmax"],"dg-publish":true,"dg-path":"人工智能/深度学习/Softmax函数.md","tags":["DL","Mathematics"],"permalink":"/人工智能/深度学习/Softmax函数/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-08-28T21:53:12.780+08:00","updated":"2025-08-30T17:23:48.000+08:00"}
---


(terminology::**Softmax Function**)
> Softmax函数，也称为归一化指数函数，是[[神经网络\|神经网络]]中常用的一种[[激活函数\|激活函数]]，尤其适用于**多分类任务的输出层**。它能够将一个包含任意实数的向量（通常是模型的原始输出，称为“logits”）转换为一个**概率分布**，其中每个元素的取值范围在 $(0, 1)$ 之间，并且所有元素的和为1。

### 核心思想：从分数到概率

在多分类问题中，[[神经网络\|神经网络]]的最后一层通常会输出一个与类别数量相等的实数向量。这些实数（logits）本身没有直接的概率意义。Softmax函数的作用就是将这些原始分数转换为易于理解的概率，使得每个类别都有一个介于0到1之间的概率值，并且所有类别的概率之和为1。

### 数学公式

对于一个包含 $K$ 个类别的输出向量 $z = [z_1, z_2, \dots, z_K]^T$，Softmax函数将其转换为一个概率分布 $P = [p_1, p_2, \dots, p_K]^T$，其中第 $j$ 个类别的概率 $p_j$ 定义为：

$$ p_j = \text{Softmax}(z_j) = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} $$

其中：
- $z_j$ 是输入向量 $z$ 中的第 $j$ 个元素（即第 $j$ 个类别的原始分数）。
- $e^{z_j}$ 是指数函数，确保所有输出都是正数。
- 分母是所有类别指数化后的和，用于归一化，确保所有概率之和为1。

### 函数图像与特性

```mermaid
graph TD
    subgraph Softmax 转换示例
        A[输入 Logits (例如: [2.0, 1.0, 0.1])] --> B{Softmax 函数};
        B --> C[输出概率分布 (例如: [0.7, 0.2, 0.1])];
    end

    style A fill:#fff,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
```

-   **概率输出**: 将任意实数转换为 $(0, 1)$ 范围内的概率值。
-   **和为1**: 所有输出概率之和严格等于1。
-   **“赢家通吃”效应**: 指数函数会放大输入值之间的差异，使得最大的输入值对应的概率更接近1，而其他较小输入值对应的概率更接近0。这有助于模型做出明确的分类决策。

### 与[[Sigmoid函数\|Sigmoid函数]]的对比

-   **[[Sigmoid函数\|Sigmoid函数]]**: 用于**二分类**问题，将单个实数输出转换为 $(0, 1)$ 范围内的概率。它独立地处理每个输出，不保证所有输出概率之和为1。
-   **Softmax函数**: 用于**多分类**问题，将一个向量的多个实数输出转换为一个概率分布，确保所有类别概率之和为1。

### 与[[交叉熵损失\|交叉熵损失]]的结合

在多分类任务中，Softmax函数通常与**[[交叉熵损失\|交叉熵损失]] (Categorical Cross-Entropy Loss)** 结合使用。Softmax将模型的原始输出转换为概率分布，而交叉熵损失则衡量这个预测概率分布与真实标签的one-hot编码分布之间的差异。这种组合在数学上非常优雅，并且在优化过程中具有良好的性质。

### 应用

-   **多分类[[神经网络\|神经网络]]的输出层**: 这是Softmax最主要的应用场景。
-   **概率建模**: 任何需要将一组分数转换为概率分布的场景。

---

> [[激活函数\|激活函数]]
> [[神经网络\|神经网络]]

```