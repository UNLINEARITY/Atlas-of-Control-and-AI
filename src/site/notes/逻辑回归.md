---
{"dg-publish":true,"dg-path":"人工智能/机器学习/逻辑回归.md","permalink":"/人工智能/机器学习/逻辑回归/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-08-31T13:05:45.000+08:00","updated":"2025-08-28T21:53:14.000+08:00"}
---


(terminology::**Logistic Regression**)  
广义[[线性回归\|线性回归]]分析，用于分类问题，尤其是二分类问题。权重向量的[[点估计\|最大近似然估计]]


逻辑回归是[[监督学习\|监督学习]] (Supervised Learning) 中一种广泛使用的[[分类\|分类]] (Classification) 算法，尽管其名称中包含“回归”，但它主要用于解决二分类问题（例如，是/否，真/假，0/1）。它通过将线性回归的输出通过一个非线性的 [[Sigmoid函数\|Sigmoid函数]] (Sigmoid Function) 映射到 0 到 1 之间，从而解释为属于某一类别的概率。


### 分类基础
分类是监督学习的最主要类型，输入变量可以是离散的，也可以是连续的
- 二分类：只需要分类1次
- 多分类：先定义其中一类为类型1（正类），其余数据为负类（rest）；接下来去掉类型1数据，剩余部分再次进行二分类。有 n 类，那就需要分类 n-1 次




### Sigmoid 函数
[[激活函数#Sigmoid\|激活函数#Sigmoid]]
Sigmoid 函数：常见的逻辑函数, S 形函数
$$\begin{align}
z=w^{T}x+b \quad \sigma (z)=g(z) =  \dfrac{1}{1+e^{-z}}\quad   g'(z)=g(z)(1-g(z))
\end{align}$$

当 $\sigma(z)\geq 0.5,y=1$ ;   当 $\sigma(z)< 0.5,y=0$ 




### 逻辑回归求解

二分类模型：$p(y\mid x;w)=(h(x))^{y}(1-h(x))^{1-y}$
逻辑函数：$z=w^{T}x+b \quad \sigma (z)=g(z) =  \dfrac{1}{1+e^{-z}}\quad   g'(z)=g(z)(1-g(z))$
损失函数： $L(\hat{y},y)=-y\log (\hat{y})-(1-y)\log(1-\hat{y})$
代价函数：$J(w)= \dfrac{1}{m} \sum\limits_{i=1}^{m}L(\hat{y},y)$


### 模型原理
逻辑回归的核心思想是：
1.  **线性组合**: 首先，像[[线性回归\|线性回归]]一样，对输入特征 $\mathbf{x}$ 进行线性组合，得到一个线性得分 $z$：
    $$z = \mathbf{w}^T \mathbf{x} + b$$
    其中 $\mathbf{w}$ 是权重向量，$b$ 是偏置项。

2.  **Sigmoid 函数**: 将线性得分 $z$ 输入到 Sigmoid 函数（也称为逻辑函数）中。Sigmoid 函数将任意实数映射到 (0, 1) 区间内，其公式为：
    $$\sigma(z) = \frac{1}{1 + e^{-z}}$$
    Sigmoid 函数的输出可以被解释为样本属于正类别（通常标记为 1）的概率 $P(y=1|\mathbf{x})$：
    $$P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b)$$
    那么，属于负类别（通常标记为 0）的概率就是：
    $$P(y=0|\mathbf{x}) = 1 - P(y=1|\mathbf{x})$$

3.  **分类决策**: 通常，如果 $P(y=1|\mathbf{x}) \ge 0.5$，则预测为正类别（1）；否则预测为负类别（0）。



### 损失函数 (Cost Function)
与线性回归使用均方误差不同，逻辑回归通常使用**交叉熵损失 (Cross-Entropy Loss)**，也称为对数损失 (Log Loss)。对于二分类问题，其损失函数定义为：

$$L(\mathbf{w}, b) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})]$$

其中 $m$ 是训练样本的数量，$y^{(i)}$ 是第 $i$ 个样本的真实标签（0 或 1），$\hat{y}^{(i)}$ 是第 $i$ 个样本预测为 1 的概率。

最小化交叉熵损失的目标是使得模型预测的概率分布尽可能接近真实标签的概率分布。

### 优化算法
逻辑回归的参数 $\mathbf{w}$ 和 $b$ 通常通过[[梯度下降\|梯度下降]] (Gradient Descent) 或其变种算法来优化，以最小化交叉熵损失函数。

### 优点与局限性

*   **优点**:
    *   简单、高效，易于实现和理解。
    *   输出结果是概率，具有很好的可解释性。
    *   对小规模数据集表现良好。
    *   可以作为许多更复杂分类算法的基线模型。
*   **局限性**:
    *   **只能处理线性可分问题**: 如果数据是非线性可分的，逻辑回归的性能会很差。
    *   对异常值敏感。
    *   无法处理多类别分类问题（需要扩展为 Softmax 回归）。
    *   特征工程对模型性能影响很大。


