---
{"dg-publish":true,"dg-path":"人工智能/机器学习/集成学习.md","permalink":"/人工智能/机器学习/集成学习/","dgPassFrontmatter":true,"noteIcon":"","created":"2025-08-06T09:50:21.900+08:00","updated":"2025-08-28T21:53:14.000+08:00"}
---

(terminology::**Ensemble Learning**)
将多个单个算法集成在一起进行建模，以提高预测的准确性和鲁棒性。
> 其核心思想是**组合多个单独的学习器（称为“基学习器”或“弱学习器”）的预测，以获得比任何单个学习器都更好的泛化性能**。


### 一、集成学习基本类型
![Pasted image 20241224005211.png](../img/user/Functional%20files/Photo%20Resources/Pasted%20image%2020241224005211.png)
#### Bagging
从训练集中进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。（民主投票）
> Bagging 通过**并行**训练多个基学习器，并让它们进行“民主投票”来做决策。其主要目标是**降低模型的方差**，从而防止[[过拟合\|过拟合]]。

**工作流程**: 
1.  **自助采样 (Bootstrap)**: 从原始训练集中有放回地随机抽取 $N$ 个样本，重复 $M$ 次，得到 $M$ 个不同的训练子集。
2.  **并行训练 (Parallel Training)**: 在这 $M$ 个训练子集上，独立地、并行地训练出 $M$ 个基学习器（通常是同一类型的模型，如决策树）。
3.  **聚合决策 (Aggregation)**: 
    - **分类任务**: 使用**投票法 (Voting)**，选择得票最多的类别作为最终结果。
    - **回归任务**: 使用**平均法 (Averaging)**，将所有基学习器的预测结果取平均值。

#### Boosting
训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果。（精英模型投票权更高）

> Boosting 通过**串行**训练一系列基学习器，每一个新的学习器都重点关注前一个学习器**预测错误的样本**。其主要目标是**降低模型的偏差**，将多个弱学习器“提升”为一个强学习器。

**工作流程**:
1.  **初始训练**: 首先训练一个初始的基学习器。
2.  **迭代训练 (Sequential Training)**: 序贯地进行 $M$ 轮迭代。在每一轮中：
    a.  **调整样本权重**: 增大上一轮被预测错误的样本的权重，减小被正确预测的样本的权重。
    b.  **训练新学习器**: 在调整了权重的训练集上，训练一个新的基学习器。这个新学习器会更加关注那些“难学的”样本。
3.  **加权聚合 (Weighted Aggregation)**: 最终的模型是所有基学习器的加权组合。通常，表现更好的基学习器会被赋予更高的权重。

**代表性算法**:
- **AdaBoost**: 第一个成功的 Boosting 算法。
- **梯度提升决策树 (Gradient Boosting Decision Tree, GBDT)**: 通过拟合前一轮学习器损失函数的负梯度来训练新的学习器，是泛化能力极强的算法。
- **XGBoost, LightGBM**: GBDT 的高效工程实现，是数据科学竞赛中的“大杀器”。

#### Stacking
元估计器+基础估计器
将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。
> Stacking 是一种分层集成方法。它训练多个不同的基学习器（例如，决策树、SVM、神经网络），然后将这些基学习器的输出作为**新的特征**，来训练一个“元学习器” (Meta-Learner)。元学习器的任务就是学习如何最好地组合这些基学习器的预测。

### 二、实际模型
#### 随机森林
(terminology::**Random Forest**)
以决策树为基学习器构建 Bagging 集成、在决策树的训练过程中引入随机特征选择
1. **随机**选择样本
2. **随机**选择特征
3. 构建决策树
4. 随机森林投票
#### Adaboost
Adaptive Boosting 自适应增强：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

基本思想：
1. 初始化训练样本的权值分布，每个样本具有相同权重；
2. 训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；
3. 将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重

后一个模型的训练永远是在前一个模型的基础上完成
![Pasted image 20241226183250.png](../img/user/Functional%20files/Photo%20Resources/Pasted%20image%2020241226183250.png)
#### GBDT 
Gradient Boosting Decision Tree
boosting    决策树
Regression Decision Tree（DT）   Gradient Boosting（ GB）    Shrinkage
#### XGBoost 
大规模并行 boosting tree 的工具，分裂方式：使用贪心方法，选增益最大的分裂方式
#### LightGBM 
LightGBM = XGBoost + GOSS + EFB+ Histogram 
主要用于解决 GDBT 在海量数据中遇到的问题，以便其可以更好更快地用于工业实践中，相对 XGBoost 具有训练速度快、内存占用低的特点。

- 基于梯度的单边采样算法 GOSS ：通过对样本采样的方法来减少计算目标函数增益时候的复杂度
- 互斥特征捆绑算法 EFB  ： 将一些特征进行融合绑定，可以降低特征数量
- 直方图算法   Histogram ：连续的特征离散化为离散特征，同时构造直方图用于统计信息
- 基于最大深度的 Leaf-wise 的垂直生长算法，选取具有最大 delta loss 的叶节点来生长
